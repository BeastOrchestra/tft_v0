{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Perform prediction using CPU\n",
    "# raw_predictions = tft.predict(\n",
    "#     val_dataloader,\n",
    "#     mode=\"raw\",\n",
    "#     return_x=True,  # Include input data for inspection\n",
    "#     trainer_kwargs={\"accelerator\": \"cpu\", \"devices\": 1}\n",
    "# )\n",
    "\n",
    "# # Unpack the Prediction object\n",
    "# output = raw_predictions[0]  # Network output\n",
    "# metadata = raw_predictions[1]  # Metadata dictionary\n",
    "\n",
    "# # Inspect the key elements\n",
    "# print(\"Network Output:\", output)\n",
    "# print(\"Metadata:\", metadata)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Extract the first sample's predictions\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m first_sample_predictions \u001b[38;5;241m=\u001b[39m \u001b[43moutput\u001b[49m\u001b[38;5;241m.\u001b[39mprediction[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Shape: (10, 1)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Convert predictions to a 1D array for plotting\u001b[39;00m\n\u001b[1;32m      7\u001b[0m predictions \u001b[38;5;241m=\u001b[39m first_sample_predictions\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mnumpy()  \u001b[38;5;66;03m# Shape: (10,)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract the first sample's predictions\n",
    "first_sample_predictions = output.prediction[0]  # Shape: (10, 1)\n",
    "\n",
    "# Convert predictions to a 1D array for plotting\n",
    "predictions = first_sample_predictions.squeeze().numpy()  # Shape: (10,)\n",
    "\n",
    "# Plot the predictions\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, len(predictions) + 1), predictions, marker='o', linestyle='-', label='Predicted Values')\n",
    "plt.title('First Sample Predictions Over Forecast Horizon', fontsize=14)\n",
    "plt.xlabel('Forecast Horizon (Time Steps)', fontsize=12)\n",
    "plt.ylabel('Predicted Value', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.legend(fontsize=10)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                    Version      Editable project location\n",
      "-------------------------- ------------ ---------------------------------------------------------------\n",
      "absl-py                    2.0.0\n",
      "aiohttp                    3.9.1\n",
      "aiosignal                  1.3.1\n",
      "antlr4-python3-runtime     4.9.3\n",
      "appdirs                    1.4.4\n",
      "appnope                    0.1.3\n",
      "arch                       5.3.1\n",
      "asttokens                  2.4.1\n",
      "async-timeout              4.0.3\n",
      "attrs                      23.2.0\n",
      "axial-positional-embedding 0.2.1\n",
      "backcall                   0.2.0\n",
      "backports.zoneinfo         0.2.1\n",
      "beautifulsoup4             4.12.3\n",
      "Brotli                     1.0.9\n",
      "cachetools                 5.3.2\n",
      "certifi                    2023.11.17\n",
      "cftime                     1.6.3\n",
      "chardet                    5.2.0\n",
      "charset-normalizer         3.3.2\n",
      "click                      8.1.7\n",
      "cmdstanpy                  0.9.68\n",
      "comm                       0.2.1\n",
      "contourpy                  1.1.1\n",
      "convertdate                2.4.0\n",
      "cramjam                    2.8.4\n",
      "cycler                     0.12.1\n",
      "Cython                     3.0.8\n",
      "debugpy                    1.6.7\n",
      "decorator                  5.1.1\n",
      "docker-pycreds             0.4.0\n",
      "einops                     0.7.0\n",
      "entrypoints                0.4\n",
      "eventkit                   1.0.3\n",
      "executing                  2.0.1\n",
      "fastparquet                2024.2.0\n",
      "filelock                   3.13.1\n",
      "fonttools                  4.47.2\n",
      "frozendict                 2.4.6\n",
      "frozenlist                 1.4.1\n",
      "fsspec                     2023.12.2\n",
      "gdown                      5.1.0\n",
      "gitdb                      4.0.11\n",
      "GitPython                  3.1.41\n",
      "google-auth                2.26.2\n",
      "google-auth-oauthlib       1.0.0\n",
      "grpcio                     1.60.0\n",
      "html5lib                   1.1\n",
      "ib-insync                  0.9.86\n",
      "idna                       3.6\n",
      "importlib-metadata         7.0.1\n",
      "importlib-resources        6.1.1\n",
      "ipykernel                  6.28.0\n",
      "ipython                    8.12.0\n",
      "jedi                       0.19.1\n",
      "Jinja2                     3.1.3\n",
      "joblib                     1.3.2\n",
      "jupyter-client             7.3.4\n",
      "jupyter_core               5.5.0\n",
      "kiwisolver                 1.4.5\n",
      "lightning                  2.3.3\n",
      "lightning-utilities        0.10.0\n",
      "local-attention            1.9.0\n",
      "lxml                       5.3.0\n",
      "Markdown                   3.5.2\n",
      "MarkupSafe                 2.1.3\n",
      "matplotlib                 3.7.4\n",
      "matplotlib-inline          0.1.6\n",
      "mplchart                   0.0.5\n",
      "mpld3                      0.5.10\n",
      "mplfinance                 0.12.10b0\n",
      "mpmath                     1.3.0\n",
      "multidict                  6.0.4\n",
      "multitasking               0.0.11\n",
      "nest-asyncio               1.5.8\n",
      "netCDF4                    1.6.5\n",
      "networkx                   2.8.8\n",
      "numpy                      1.24.3\n",
      "nystrom-attention          0.0.11\n",
      "oauthlib                   3.2.2\n",
      "omegaconf                  2.3.0\n",
      "opencv-python              4.9.0.80\n",
      "opt-einsum                 3.3.0\n",
      "packaging                  23.2\n",
      "pandas                     2.0.3\n",
      "parso                      0.8.3\n",
      "patsy                      0.5.6\n",
      "peewee                     3.17.8\n",
      "performer-pytorch          1.1.4\n",
      "pexpect                    4.8.0\n",
      "pickleshare                0.7.5\n",
      "Pillow                     9.4.0\n",
      "pip                        23.3.1\n",
      "platformdirs               4.1.0\n",
      "prompt-toolkit             3.0.42\n",
      "property-cached            1.6.4\n",
      "protobuf                   4.25.2\n",
      "psutil                     5.9.0\n",
      "ptyprocess                 0.7.0\n",
      "pure-eval                  0.2.2\n",
      "pyarrow                    17.0.0\n",
      "pyasn1                     0.5.1\n",
      "pyasn1-modules             0.3.0\n",
      "pyDeprecate                0.3.2\n",
      "Pygments                   2.17.2\n",
      "PyMeeus                    0.5.12\n",
      "pyparsing                  3.1.1\n",
      "PySocks                    1.7.1\n",
      "pystan                     2.19.1.1\n",
      "python-dateutil            2.8.2\n",
      "pytorch-forecasting        1.1.1\n",
      "pytorch-lightning          2.1.3\n",
      "pytz                       2023.3.post1\n",
      "PyYAML                     5.4.1\n",
      "pyzmq                      25.1.0\n",
      "requests                   2.31.0\n",
      "requests-oauthlib          1.3.1\n",
      "rsa                        4.9\n",
      "scikit-learn               1.3.2\n",
      "scipy                      1.10.1\n",
      "seaborn                    0.13.1\n",
      "sentry-sdk                 1.39.2\n",
      "setproctitle               1.3.3\n",
      "setuptools                 68.2.2\n",
      "six                        1.16.0\n",
      "smmap                      5.0.1\n",
      "soupsieve                  2.5\n",
      "spacetimeformer            1.5.0        /Users/alecjeffery/Documents/Playgrounds/Python/spacetimeformer\n",
      "stack-data                 0.6.2\n",
      "statsmodels                0.14.1\n",
      "sympy                      1.12\n",
      "ta                         0.11.0\n",
      "tensorboard                2.14.0\n",
      "tensorboard-data-server    0.7.2\n",
      "threadpoolctl              3.2.0\n",
      "torch                      2.1.2\n",
      "torchaudio                 2.1.2\n",
      "torchmetrics               1.3.0\n",
      "torchvision                0.16.2\n",
      "tornado                    6.1\n",
      "tqdm                       4.66.1\n",
      "traitlets                  5.14.1\n",
      "typing_extensions          4.9.0\n",
      "tzdata                     2023.4\n",
      "ujson                      5.9.0\n",
      "urllib3                    2.1.0\n",
      "wandb                      0.16.2\n",
      "wcwidth                    0.2.13\n",
      "webencodings               0.5.1\n",
      "Werkzeug                   3.0.1\n",
      "wheel                      0.41.2\n",
      "yarl                       1.9.4\n",
      "yfinance                   0.2.49\n",
      "zipp                       3.17.0\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to an error with the 'pyzmq' module. Consider re-installing this module.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresPyzmq'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from pytorch_forecasting.models import TemporalFusionTransformer\n",
    "# from pytorch_forecasting.data import TimeSeriesDataSet\n",
    "# from pytorch_forecasting.metrics import SMAPE, MAE\n",
    "# from pytorch_lightning.callbacks import Callback\n",
    "# from torch.utils.data import DataLoader\n",
    "# import torch\n",
    "# from pytorch_lightning import Trainer, LightningModule\n",
    "\n",
    "# # Set device to CUDA if available, otherwise fall back to CPU\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(f\"Using device: {device}\")\n",
    "\n",
    "# os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "# # Define a custom callback to log train and validation loss\n",
    "# class LossLoggerCallback(Callback):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.epoch_losses = []\n",
    "\n",
    "#     def on_train_epoch_end(self, trainer, pl_module):\n",
    "#         train_loss = trainer.callback_metrics.get(\"train_loss\", None)\n",
    "#         val_loss = trainer.callback_metrics.get(\"val_loss\", None)\n",
    "#         self.epoch_losses.append((trainer.current_epoch, train_loss, val_loss))\n",
    "#         print(f\"Epoch {trainer.current_epoch} -> Train Loss: {train_loss}, Validation Loss: {val_loss}\")\n",
    "\n",
    "\n",
    "# # Dataset preparation class\n",
    "# class TimeSeriesDataset:\n",
    "#     def __init__(self, data_folder, context_length, forecast_length):\n",
    "#         self.context_length = context_length\n",
    "#         self.forecast_length = forecast_length\n",
    "#         self.data_files = self.load_csv_files(data_folder)\n",
    "\n",
    "#     def load_csv_files(self, folder):\n",
    "#         all_files = [os.path.join(folder, file) for file in os.listdir(folder) if file.endswith('.csv')]\n",
    "#         datasets = []\n",
    "#         for file in all_files:\n",
    "#             df = pd.read_csv(file, index_col=0)\n",
    "#             df[\"time_idx\"] = range(len(df))\n",
    "#             df[\"group\"] = os.path.basename(file).split('.')[0]  # Use the file name (without extension) as group ID\n",
    "#             datasets.append(df)\n",
    "#         return datasets\n",
    "\n",
    "#     def to_timeseries_dataset(self):\n",
    "#         combined_df = pd.concat(self.data_files, ignore_index=True)\n",
    "#         return combined_df\n",
    "\n",
    "# # LightningModule wrapper for TemporalFusionTransformer\n",
    "# # Wrap the TFT model in the LightningModule\n",
    "# class TFTLightningModule(LightningModule):\n",
    "#     def __init__(self, tft_model):\n",
    "#         super().__init__()\n",
    "#         self.tft_model = tft_model\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.tft_model(x)\n",
    "\n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         x, y = batch\n",
    "#         x = {key: value.to(self.device) for key, value in x.items()}\n",
    "\n",
    "#         # Extract the actual target tensor from the tuple\n",
    "#         y = y[0].to(self.device)\n",
    "\n",
    "#         # Forward pass\n",
    "#         y_hat = self.tft_model(x)\n",
    "#         y_pred = y_hat[\"prediction\"]\n",
    "\n",
    "#         # Reshape predictions if necessary\n",
    "#         if y_pred.ndim > 3:\n",
    "#             y_pred = y_pred.squeeze(-1)\n",
    "\n",
    "#         # Compute loss\n",
    "#         loss = self.tft_model.loss(y_pred, y)\n",
    "#         self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "#         return loss\n",
    "\n",
    "#     def validation_step(self, batch, batch_idx):\n",
    "#         x, y = batch\n",
    "#         x = {key: value.to(self.device) for key, value in x.items()}\n",
    "\n",
    "#         # Extract the actual target tensor from the tuple\n",
    "#         y = y[0].to(self.device)\n",
    "\n",
    "#         # Forward pass\n",
    "#         y_hat = self.tft_model(x)\n",
    "#         y_pred = y_hat[\"prediction\"]\n",
    "\n",
    "#         # Reshape predictions if necessary\n",
    "#         if y_pred.ndim > 3:\n",
    "#             y_pred = y_pred.squeeze(-1)\n",
    "\n",
    "#         # Compute loss\n",
    "#         loss = self.tft_model.loss(y_pred, y)\n",
    "#         self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "#         return loss\n",
    "\n",
    "#     def configure_optimizers(self):\n",
    "#         return torch.optim.Adam(self.parameters(), lr=self.tft_model.hparams.learning_rate)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Paths for train and test data\n",
    "#     train_data_folder = \"/Users/alecjeffery/Documents/Playgrounds/Python/spacetimeformer_stocks/spacetimeformer/data/train\"\n",
    "#     test_data_folder = \"/Users/alecjeffery/Documents/Playgrounds/Python/spacetimeformer_stocks/spacetimeformer/data/test\"\n",
    "\n",
    "#     context_length = 90\n",
    "#     forecast_length = 10\n",
    "\n",
    "#     # Prepare the train dataset\n",
    "#     train_dataset = TimeSeriesDataset(train_data_folder, context_length, forecast_length)\n",
    "#     train_combined_data = train_dataset.to_timeseries_dataset()\n",
    "#     # Prepare the test dataset\n",
    "#     test_dataset = TimeSeriesDataset(test_data_folder, context_length, forecast_length)\n",
    "#     test_combined_data = test_dataset.to_timeseries_dataset()\n",
    "\n",
    "#     # Ensure 'group' remains a string type\n",
    "#     train_combined_data[\"group\"] = train_combined_data[\"group\"].astype(str)\n",
    "#     test_combined_data[\"group\"] = test_combined_data[\"group\"].astype(str)\n",
    "\n",
    "#     # # Specify additional dataset columns for train\n",
    "#     # train_combined_data[\"target\"] = train_combined_data.iloc[:, 3]  # Assuming the target column is the fourth column\n",
    "#     # # Specify additional dataset columns for test\n",
    "#     # test_combined_data[\"target\"] = test_combined_data.iloc[:, 3]  # Assuming the target column is the fourth column\n",
    "\n",
    "#     train_combined_data = train_combined_data.rename(columns={\"Close\": \"target\"})\n",
    "#     test_combined_data = test_combined_data.rename(columns={\"Close\": \"target\"})\n",
    "\n",
    "\n",
    "#     # Define the TimeSeriesDataSet for train\n",
    "#     train_ts_dataset = TimeSeriesDataSet(\n",
    "#         train_combined_data,\n",
    "#         time_idx=\"time_idx\",\n",
    "#         target=\"target\",\n",
    "#         group_ids=[\"group\"],\n",
    "#         max_encoder_length=context_length,\n",
    "#         max_prediction_length=forecast_length,\n",
    "#         static_categoricals=[\"group\"],  # 'group' remains a categorical string\n",
    "#         time_varying_known_reals=[\"time_idx\"],\n",
    "#         time_varying_unknown_reals = train_combined_data.drop(columns=['group', 'time_idx']).columns.tolist(),\n",
    "#         # time_varying_unknown_reals=train_combined_data.columns[:].tolist(),#[\"target\"],\n",
    "#         target_normalizer=None,\n",
    "#     )\n",
    "\n",
    "#     # Define the TimeSeriesDataSet for validation\n",
    "#     val_ts_dataset = TimeSeriesDataSet(\n",
    "#         test_combined_data,\n",
    "#         time_idx=\"time_idx\",\n",
    "#         target=\"target\",\n",
    "#         group_ids=[\"group\"],  # 'group' remains a categorical string\n",
    "#         max_encoder_length=context_length,\n",
    "#         max_prediction_length=forecast_length,\n",
    "#         static_categoricals=[\"group\"],\n",
    "#         time_varying_known_reals=[\"time_idx\"],\n",
    "#         # time_varying_unknown_reals=test_combined_data.columns[:].tolist(),\n",
    "#         time_varying_unknown_reals = test_combined_data.drop(columns=['group', 'time_idx']).columns.tolist(),\n",
    "#         target_normalizer=None,\n",
    "#     )\n",
    "\n",
    "#     # Split into dataloaders\n",
    "#     train_dataloader = train_ts_dataset.to_dataloader(\n",
    "#         train=True, batch_size=64, shuffle=True, num_workers=4, persistent_workers=True\n",
    "#     )\n",
    "#     val_dataloader = val_ts_dataset.to_dataloader(\n",
    "#         train=False, batch_size=64, shuffle=False, num_workers=4, persistent_workers=True\n",
    "#     )\n",
    "\n",
    "#     # Define the TemporalFusionTransformer model\n",
    "#     tft = TemporalFusionTransformer.from_dataset(\n",
    "#         train_ts_dataset,\n",
    "#         learning_rate=1e-4,\n",
    "#         lstm_layers=2,\n",
    "#         hidden_size=32,  # 16\n",
    "#         # n_heads=4,\n",
    "#         attention_head_size=16,\n",
    "#         dropout=0.2,\n",
    "#         # max_encoder_length=90,\n",
    "#         # max_prediction_length=10,\n",
    "#         hidden_continuous_size=16,\n",
    "#         output_size=1,  # Single target output\n",
    "#         loss=MAE(),#SMAPE(),\n",
    "#         log_interval=10,\n",
    "#         reduce_on_plateau_patience=4,\n",
    "#     )\n",
    "\n",
    "#     # Wrap the TFT model in the LightningModule\n",
    "#     tft_module = TFTLightningModule(tft)\n",
    "#     tft_module = tft_module.to(device)\n",
    "\n",
    "#     # Print model size\n",
    "#     print(f\"Number of parameters in model: {tft.size()}\")\n",
    "\n",
    "#     # Use PyTorch Lightning Trainer for training\n",
    "#     trainer = Trainer(\n",
    "#         max_epochs=10,\n",
    "#         gradient_clip_val=0.1,\n",
    "#         enable_checkpointing=True,\n",
    "#         callbacks=[LossLoggerCallback()], \n",
    "#         accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "#         devices=1,  # Number of GPUs to use\n",
    "#     )\n",
    "\n",
    "#     # Train the model\n",
    "#     trainer.fit(tft_module, train_dataloader, val_dataloader)\n",
    "\n",
    "#     # Save the trained model\n",
    "#     torch.save(tft_module.state_dict(), \"tft_model.pth\")\n",
    "\n",
    "#     # # Example inference (requires additional implementation for DataLoader processing)\n",
    "#     # raw_predictions, x = tft.predict(val_dataloader, mode=\"raw\", return_x=True)\n",
    "#     # print(\"Raw predictions:\", raw_predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OOS predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to an error with the 'pyzmq' module. Consider re-installing this module.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresPyzmq'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# from pytorch_forecasting.data import TimeSeriesDataSet\n",
    "\n",
    "# # Define the directory holding OOS data\n",
    "# oos_data_folder = \"/Users/alecjeffery/Documents/Playgrounds/Python/spacetimeformer_stocks/spacetimeformer/data/oos/\"  # Update with the actual path to your OOS directory\n",
    "\n",
    "# # Load all CSV files in the OOS directory\n",
    "# oos_files = [os.path.join(oos_data_folder, f) for f in os.listdir(oos_data_folder) if f.endswith('.csv')]\n",
    "\n",
    "# # Process each file\n",
    "# for file_path in oos_files:\n",
    "#     # Load OOS data\n",
    "#     oos_df = pd.read_csv(file_path, index_col=0)\n",
    "#     oos_df[\"time_idx\"] = range(len(oos_df))  # Ensure `time_idx` is correctly set\n",
    "#     oos_df[\"group\"] = os.path.basename(file_path).split('.')[0]  # Use filename as group ID\n",
    "    \n",
    "#     # Assuming the target column is the second column in the dataset\n",
    "#     oos_df[\"target\"] = oos_df.iloc[:, 4]\n",
    "    \n",
    "#     # Prepare TimeSeriesDataSet for OOS data\n",
    "#     oos_ts_dataset = TimeSeriesDataSet(\n",
    "#         oos_df,\n",
    "#         time_idx=\"time_idx\",\n",
    "#         target=\"target\",\n",
    "#         group_ids=[\"group\"],\n",
    "#         max_encoder_length=30,  # Context window (your encoder length)\n",
    "#         max_prediction_length=10,  # Forecast window\n",
    "#         static_categoricals=[\"group\"],\n",
    "#         time_varying_known_reals=[\"time_idx\"],\n",
    "#         time_varying_unknown_reals=[\"target\"],\n",
    "#         target_normalizer=None,\n",
    "#     )\n",
    "    \n",
    "#     # Create a DataLoader\n",
    "#     oos_dataloader = oos_ts_dataset.to_dataloader(\n",
    "#         train=False, batch_size=64, shuffle=False, num_workers=4, persistent_workers=True\n",
    "#     )\n",
    "    \n",
    "#     # Make predictions\n",
    "#     raw_predictions = tft.predict(\n",
    "#         oos_dataloader,\n",
    "#         mode=\"raw\",\n",
    "#         return_x=False,  # We don't need the input data here\n",
    "#         trainer_kwargs={\"accelerator\": \"cpu\", \"devices\": 1},\n",
    "#     )\n",
    "    \n",
    "#     last_batch_predictions = raw_predictions.prediction[-1]  # The last batch\n",
    "\n",
    "#     # Extract the predictions for the last context window\n",
    "#     final_forecast = last_batch_predictions[-10:]  # All ten forecast periods for the final sample\n",
    "#     print(f\"File: {os.path.basename(file_path)}\")\n",
    "#     print(f\"Final Forecast (Last Context Window -> 10 Periods Ahead): {final_forecast.squeeze().tolist()}\")\n",
    "#     print(\"-\" * 40)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alecjeffery/anaconda3/envs/spacetimeformer/lib/python3.8/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/alecjeffery/anaconda3/envs/spacetimeformer/lib/python3.8/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/alecjeffery/anaconda3/envs/spacetimeformer/lib/python3.8/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/alecjeffery/anaconda3/envs/spacetimeformer/lib/python3.8/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/alecjeffery/anaconda3/envs/spacetimeformer/lib/python3.8/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/alecjeffery/anaconda3/envs/spacetimeformer/lib/python3.8/site-packages/lightning/pytorch/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "/Users/alecjeffery/anaconda3/envs/spacetimeformer/lib/python3.8/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/alecjeffery/anaconda3/envs/spacetimeformer/lib/python3.8/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/alecjeffery/anaconda3/envs/spacetimeformer/lib/python3.8/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/alecjeffery/anaconda3/envs/spacetimeformer/lib/python3.8/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/alecjeffery/anaconda3/envs/spacetimeformer/lib/python3.8/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/alecjeffery/anaconda3/envs/spacetimeformer/lib/python3.8/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/alecjeffery/anaconda3/envs/spacetimeformer/lib/python3.8/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/alecjeffery/anaconda3/envs/spacetimeformer/lib/python3.8/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/alecjeffery/anaconda3/envs/spacetimeformer/lib/python3.8/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/alecjeffery/anaconda3/envs/spacetimeformer/lib/python3.8/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/alecjeffery/anaconda3/envs/spacetimeformer/lib/python3.8/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/alecjeffery/anaconda3/envs/spacetimeformer/lib/python3.8/site-packages/lightning/pytorch/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'prediction'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 52\u001b[0m\n\u001b[1;32m     44\u001b[0m raw_predictions \u001b[38;5;241m=\u001b[39m tft\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[1;32m     45\u001b[0m     oos_dataloader,\n\u001b[1;32m     46\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     47\u001b[0m     return_x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# We don't need the input data here\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     trainer_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccelerator\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevices\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m},\n\u001b[1;32m     49\u001b[0m )\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Extract the last predictions for this file\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m last_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mraw_predictions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction\u001b[49m[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Extract last time step for each sample\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Store results\u001b[39;00m\n\u001b[1;32m     55\u001b[0m oos_predictions_results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m: os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(file_path),\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlast_predictions\u001b[39m\u001b[38;5;124m\"\u001b[39m: last_predictions\u001b[38;5;241m.\u001b[39mtolist()  \u001b[38;5;66;03m# Convert tensor to list for better readability\u001b[39;00m\n\u001b[1;32m     58\u001b[0m })\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'prediction'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pytorch_forecasting.data import TimeSeriesDataSet\n",
    "\n",
    "# Define the directory holding OOS data\n",
    "oos_data_folder = \"/Users/alecjeffery/Documents/Playgrounds/Python/spacetimeformer_stocks/spacetimeformer/data/oos/\"  # Update with the actual path to your OOS directory\n",
    "# Load all CSV files in the OOS directory\n",
    "oos_files = [os.path.join(oos_data_folder, f) for f in os.listdir(oos_data_folder) if f.endswith('.csv')]\n",
    "\n",
    "# Initialize a list to store results\n",
    "oos_predictions_results = []\n",
    "\n",
    "# Process each file\n",
    "for file_path in oos_files:\n",
    "    # Load OOS data\n",
    "    oos_df = pd.read_csv(file_path, index_col=0)\n",
    "    oos_df[\"time_idx\"] = range(len(oos_df))  # Ensure `time_idx` is correctly set\n",
    "    oos_df[\"group\"] = os.path.basename(file_path).split('.')[0]  # Use filename as group ID\n",
    "    \n",
    "    # Assuming the target column is the second column in the dataset\n",
    "    oos_df[\"target\"] = oos_df.iloc[:, 4]\n",
    "    \n",
    "    # Prepare TimeSeriesDataSet for OOS data\n",
    "    oos_ts_dataset = TimeSeriesDataSet(\n",
    "        oos_df,\n",
    "        time_idx=\"time_idx\",\n",
    "        target=\"target\",\n",
    "        group_ids=[\"group\"],\n",
    "        max_encoder_length=context_length,  # Use the same context length as in training\n",
    "        max_prediction_length=forecast_length,  # Use the same forecast length as in training\n",
    "        static_categoricals=[\"group\"],\n",
    "        time_varying_known_reals=[\"time_idx\"],\n",
    "        time_varying_unknown_reals=[\"target\"],\n",
    "        target_normalizer=None,\n",
    "    )\n",
    "    \n",
    "    # Create a DataLoader\n",
    "    oos_dataloader = oos_ts_dataset.to_dataloader(\n",
    "        train=False, batch_size=64, shuffle=False, num_workers=4, persistent_workers=True\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    raw_predictions = tft.predict(\n",
    "        oos_dataloader,\n",
    "        mode=\"raw\",\n",
    "        return_x=False,  # We don't need the input data here\n",
    "        trainer_kwargs={\"accelerator\": \"cpu\", \"devices\": 1},\n",
    "    )\n",
    "    \n",
    "    # Extract the last predictions for this file\n",
    "    last_predictions = raw_predictions.prediction[:, -1, 0]  # Extract last time step for each sample\n",
    "    \n",
    "    # Store results\n",
    "    oos_predictions_results.append({\n",
    "        \"file\": os.path.basename(file_path),\n",
    "        \"last_predictions\": last_predictions.tolist()  # Convert tensor to list for better readability\n",
    "    })\n",
    "\n",
    "# Print results\n",
    "for result in oos_predictions_results:\n",
    "    print(f\"File: {result['file']}\")\n",
    "    print(f\"Last Predictions: {result['last_predictions']}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alecjeffery/Documents/Playgrounds/Python/tft_v0/.venv/lib/python3.10/site-packages/pytorch_forecasting/models/base_model.py:27: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       open      high       low  target_1  target_2     vopen     vhigh  \\\n",
      "0 -1.096703 -1.135595 -1.102126 -1.080038  0.905195  1.462900  1.137046   \n",
      "1 -1.151305 -1.132208 -1.145358 -1.082914  0.868557  1.133347  1.051088   \n",
      "2 -1.107537 -1.117413 -1.153477 -1.131560  0.656849  0.918255  0.743172   \n",
      "3 -1.316876 -1.337029 -1.431986 -1.442457  2.534582  1.298492  2.311808   \n",
      "4 -1.455011 -1.331863 -1.420019 -1.401781  2.454930  2.681430  2.294686   \n",
      "\n",
      "       vlow       VIX       SPY  ...  close_spy_corr65  close_tnx_corr65  \\\n",
      "0  1.082468  2.661094 -0.574630  ...          1.005373          0.630191   \n",
      "1  1.032907  2.950700 -0.726973  ...          0.920472          0.506773   \n",
      "2  0.773295  2.577170 -0.724573  ...          0.857915          0.411523   \n",
      "3  1.565795  5.020879 -1.388272  ...          0.856561          0.417821   \n",
      "4  2.343257  4.836254 -1.605435  ...          0.833293          0.407856   \n",
      "\n",
      "   vclose_VIX_corr65  garch_IV_corr65  close_spy_corr252  close_tnx_corr252  \\\n",
      "0           1.563984         1.344441          -0.826058          -0.979360   \n",
      "1           1.524565         1.282873          -0.807958          -0.943066   \n",
      "2           1.493687         1.233097          -0.789398          -0.904332   \n",
      "3           1.467626         1.343792          -0.756445          -0.853819   \n",
      "4           1.444498         1.428037          -0.720182          -0.806063   \n",
      "\n",
      "   vclose_VIX_corr252  garch_IV_corr252  time_idx  group  \n",
      "0           -0.488543         -0.744321         0   CSCO  \n",
      "1           -0.500879         -0.706505         1   CSCO  \n",
      "2           -0.513641         -0.704123         2   CSCO  \n",
      "3           -0.447295         -0.596371         3   CSCO  \n",
      "4           -0.378645         -0.469857         4   CSCO  \n",
      "\n",
      "[5 rows x 97 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pytorch_forecasting.data import TimeSeriesDataSet\n",
    "\n",
    "# Function to load and preprocess CSV files\n",
    "def load_data(folder):\n",
    "    \"\"\"\n",
    "    Loads CSV files from a given folder, processes them into a format suitable for PyTorch Forecasting.\n",
    "    \n",
    "    Args:\n",
    "        folder (str): Path to the folder containing CSV files.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Processed DataFrame with necessary columns.\n",
    "    \"\"\"\n",
    "    all_files = [os.path.join(folder, file) for file in os.listdir(folder) if file.endswith('.csv')]\n",
    "    datasets = []\n",
    "    \n",
    "    for file in all_files:\n",
    "        df = pd.read_csv(file, index_col=0)\n",
    "        \n",
    "        # Assign sequential time index for each file\n",
    "        df[\"time_idx\"] = range(len(df))\n",
    "        \n",
    "        # Extract stock ticker from filename and assign it as 'group' and 'group_ids'\n",
    "        stock_ticker = os.path.basename(file).split('.')[0]\n",
    "        df[\"group\"] = stock_ticker\n",
    "        \n",
    "        # Rename target columns\n",
    "        df = df.rename(columns={\"Close\": \"target_1\", \"vclose\": \"target_2\"})\n",
    "        \n",
    "        # Convert categorical fields to string\n",
    "        df[\"group\"] = df[\"group\"].astype(str)\n",
    "        \n",
    "        datasets.append(df)\n",
    "    \n",
    "    # Combine all datasets into one DataFrame\n",
    "    return pd.concat(datasets, ignore_index=True)\n",
    "\n",
    "# Define data folders\n",
    "train_data_folder = \"data/train\"\n",
    "test_data_folder = \"data/test\"\n",
    "oos_data_folder = \"data/oos\"\n",
    "\n",
    "# Load datasets\n",
    "train_df = load_data(train_data_folder)\n",
    "test_df = load_data(test_data_folder)\n",
    "oos_df = load_data(oos_data_folder)\n",
    "\n",
    "# Display data sample\n",
    "print(train_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define TimeSeriesDataSet parameters\n",
    "max_encoder_length = 30  # Number of historical timesteps to use\n",
    "max_prediction_length = 10  # Number of timesteps to predict into the future\n",
    "\n",
    "# Create TimeSeriesDataSet for training\n",
    "training = TimeSeriesDataSet(\n",
    "    train_df,\n",
    "    time_idx=\"time_idx\",\n",
    "    target=[\"target_1\", \"target_2\"],\n",
    "    group_ids=[\"group\"],  # Static identifier\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=[\"group\"],  # Stock ticker as a categorical feature\n",
    "    time_varying_known_reals=[\"time_idx\"],  # Future known variables\n",
    "    time_varying_unknown_reals=[\"target_1\", \"target_2\"],  # Targets for prediction\n",
    ")\n",
    "\n",
    "# Create validation dataset\n",
    "validation = TimeSeriesDataSet.from_dataset(training, test_df)\n",
    "\n",
    "# Create OOS dataset\n",
    "oos = TimeSeriesDataSet.from_dataset(training, oos_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create training DataLoader\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=64, num_workers=4)\n",
    "\n",
    "# Create validation DataLoader\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=64, num_workers=4)\n",
    "\n",
    "# Create validation DataLoader\n",
    "oos_dataloader = oos.to_dataloader(train=False, batch_size=64, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/alecjeffery/Documents/Playgrounds/Python/tft_v0/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/alecjeffery/Documents/Playgrounds/Python/tft_v0/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "# from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "from pytorch_forecasting.models import TemporalFusionTransformer\n",
    "\n",
    "# Callbacks for early stopping and logging\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", patience=10, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()\n",
    "# logger = TensorBoardLogger(\"lightning_logs\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,\n",
    "    accelerator=\"cpu\",  # Change to \"gpu\" if using a GPU\n",
    "    enable_model_summary=True,\n",
    "    gradient_clip_val=0.1,\n",
    "    limit_train_batches=50,  # Uncomment for debugging\n",
    "    callbacks=[lr_logger, early_stop_callback],\n",
    "    # logger=logger,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in network: 17.9k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alecjeffery/Documents/Playgrounds/Python/tft_v0/.venv/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "/Users/alecjeffery/Documents/Playgrounds/Python/tft_v0/.venv/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "`model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `TemporalFusionTransformer`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 17\u001b[0m\n\u001b[1;32m      2\u001b[0m tft \u001b[38;5;241m=\u001b[39m TemporalFusionTransformer\u001b[38;5;241m.\u001b[39mfrom_dataset(\n\u001b[1;32m      3\u001b[0m     training,\n\u001b[1;32m      4\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.03\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     reduce_on_plateau_patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of parameters in network: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtft\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1e3\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Playgrounds/Python/tft_v0/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:533\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    501\u001b[0m     model: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    505\u001b[0m     ckpt_path: Optional[_PATH] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    506\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    507\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Runs the full optimization routine.\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \n\u001b[1;32m    509\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    531\u001b[0m \n\u001b[1;32m    532\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 533\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43m_maybe_unwrap_optimized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    534\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m    535\u001b[0m     _verify_strategy_supports_compile(model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy)\n",
      "File \u001b[0;32m~/Documents/Playgrounds/Python/tft_v0/.venv/lib/python3.10/site-packages/pytorch_lightning/utilities/compile.py:111\u001b[0m, in \u001b[0;36m_maybe_unwrap_optimized\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m    110\u001b[0m _check_mixed_imports(model)\n\u001b[0;32m--> 111\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(model)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    113\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: `model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `TemporalFusionTransformer`"
     ]
    }
   ],
   "source": [
    "# Initialize the TFT model\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=16,  # LSTM hidden size\n",
    "    attention_head_size=2,  # Number of attention heads\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=8,  # Embedding size for continuous variables\n",
    "    loss=QuantileLoss(),\n",
    "    log_interval=10,  # Log every 10 batches\n",
    "    optimizer=\"Ranger\",  # Uses the Ranger optimizer\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "\n",
    "print(f\"Number of parameters in network: {tft.size() / 1e3:.1f}k\")\n",
    "\n",
    "trainer.fit(tft, train_dataloader, val_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in network: 17.9k\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Optimizer of self.hparams.optimizer=Ranger unknown",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 57\u001b[0m\n\u001b[1;32m     49\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     50\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m     51\u001b[0m     accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     52\u001b[0m     devices\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     53\u001b[0m     precision\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m16-mixed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m32\u001b[39m,  \u001b[38;5;66;03m# Mixed precision for efficiency\u001b[39;00m\n\u001b[1;32m     54\u001b[0m )\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m#  Now train the wrapped model\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtft_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/spacetimeformer/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/spacetimeformer/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/anaconda3/envs/spacetimeformer/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    574\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    576\u001b[0m     ckpt_path,\n\u001b[1;32m    577\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    579\u001b[0m )\n\u001b[0;32m--> 580\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/spacetimeformer/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:965\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger_connector\u001b[38;5;241m.\u001b[39mreset_metrics()\n\u001b[1;32m    964\u001b[0m \u001b[38;5;66;03m# strategy will configure model and move it to the device\u001b[39;00m\n\u001b[0;32m--> 965\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[38;5;66;03m# hook\u001b[39;00m\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;241m==\u001b[39m TrainerFn\u001b[38;5;241m.\u001b[39mFITTING:\n",
      "File \u001b[0;32m~/anaconda3/envs/spacetimeformer/lib/python3.8/site-packages/pytorch_lightning/strategies/single_device.py:78\u001b[0m, in \u001b[0;36mSingleDeviceStrategy.setup\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetup\u001b[39m(\u001b[38;5;28mself\u001b[39m, trainer: pl\u001b[38;5;241m.\u001b[39mTrainer) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_to_device()\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/spacetimeformer/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py:151\u001b[0m, in \u001b[0;36mStrategy.setup\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39msetup(trainer)\n\u001b[0;32m--> 151\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup_optimizers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetup_precision_plugin()\n\u001b[1;32m    153\u001b[0m _optimizers_to_device(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/spacetimeformer/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py:140\u001b[0m, in \u001b[0;36mStrategy.setup_optimizers\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler_configs \u001b[38;5;241m=\u001b[39m \u001b[43m_init_optimizers_and_lr_schedulers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/spacetimeformer/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py:177\u001b[0m, in \u001b[0;36m_init_optimizers_and_lr_schedulers\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls `LightningModule.configure_optimizers` and parses and validates the output.\"\"\"\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m call\n\u001b[0;32m--> 177\u001b[0m optim_conf \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfigure_optimizers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpl_module\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optim_conf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    180\u001b[0m     rank_zero_warn(\n\u001b[1;32m    181\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    182\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/spacetimeformer/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py:157\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m hook_name\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 157\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    160\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "Cell \u001b[0;32mIn[28], line 43\u001b[0m, in \u001b[0;36mTFTLightningWrapper.configure_optimizers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconfigure_optimizers\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfigure_optimizers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/spacetimeformer/lib/python3.8/site-packages/pytorch_forecasting/models/base_model.py:1259\u001b[0m, in \u001b[0;36mBaseModel.configure_optimizers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1257\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer of self.hparams.optimizer=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhparams\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m unknown\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1258\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1259\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer of self.hparams.optimizer=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhparams\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m unknown\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;66;03m# set scheduler\u001b[39;00m\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(lrs, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):  \u001b[38;5;66;03m# change for each epoch\u001b[39;00m\n\u001b[1;32m   1263\u001b[0m     \u001b[38;5;66;03m# normalize lrs\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Optimizer of self.hparams.optimizer=Ranger unknown"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_forecasting import TemporalFusionTransformer\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "\n",
    "# Initialize the TFT model\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=16,  # LSTM hidden size\n",
    "    attention_head_size=2,  # Number of attention heads\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=8,  # Embedding size for continuous variables\n",
    "    loss=QuantileLoss(),\n",
    "    log_interval=10,  # Log every 10 batches\n",
    "    optimizer=\"Ranger\",  # Uses the Ranger optimizer\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "\n",
    "print(f\"Number of parameters in network: {tft.size() / 1e3:.1f}k\")\n",
    "\n",
    "#  Wrap the TFT model in a LightningModule\n",
    "class TFTLightningWrapper(LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        y_hat, _ = self.model(batch)\n",
    "        loss = self.model.loss(y_hat, batch[\"target\"])\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        y_hat, _ = self.model(batch)\n",
    "        loss = self.model.loss(y_hat, batch[\"target\"])\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return self.model.configure_optimizers()\n",
    "\n",
    "#  Use the wrapped model\n",
    "tft_model = TFTLightningWrapper(tft)\n",
    "\n",
    "# Trainer Configuration\n",
    "trainer = Trainer(\n",
    "    max_epochs=50,\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    devices=1,\n",
    "    precision=\"16-mixed\" if torch.cuda.is_available() else 32,  # Mixed precision for efficiency\n",
    ")\n",
    "\n",
    "#  Now train the wrapped model\n",
    "trainer.fit(tft_model, train_dataloader, val_dataloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will work for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alecjeffery/Documents/Playgrounds/Python/tft_v0/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "/Users/alecjeffery/Documents/Playgrounds/Python/tft_v0/.venv/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "/Users/alecjeffery/Documents/Playgrounds/Python/tft_v0/.venv/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "/Users/alecjeffery/Documents/Playgrounds/Python/tft_v0/.venv/lib/python3.10/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/__init__.py:171: UserWarning: In pytorch-forecasting models, on versions 1.1.X, the default optimizer defaults to 'adam', if pytorch_optimizer is not installed, otherwise it defaults to 'ranger' from pytorch_optimizer. From version 1.2.0, the default optimizer will be 'adam' regardless of whether pytorch_optimizer is installed, in order to minimize the number of dependencies in default parameter settings. Users who wish to ensure their code continues using 'ranger' as optimizer should ensure that pytorch_optimizer is installed, and set the optimizer parameter explicitly to 'ranger'.\n",
      "  super().__init__(loss=loss, logging_metrics=logging_metrics, **kwargs)\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/alecjeffery/Documents/Playgrounds/Python/tft_v0/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name      | Type                      | Params | Mode \n",
      "----------------------------------------------------------------\n",
      "0 | tft_model | TemporalFusionTransformer | 2.5 M  | train\n",
      "----------------------------------------------------------------\n",
      "2.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.5 M     Total params\n",
      "10.029    Total estimated model params size (MB)\n",
      "1650      Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params in network: 2507.3k\n",
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alecjeffery/Documents/Playgrounds/Python/tft_v0/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:420: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alecjeffery/Documents/Playgrounds/Python/tft_v0/.venv/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 16. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "/Users/alecjeffery/Documents/Playgrounds/Python/tft_v0/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:420: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|| 13229/13229 [1:31:17<00:00,  2.42it/s, v_num=0, train_loss_step=0.994, val_loss=0.907, train_loss_epoch=0.857]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alecjeffery/Documents/Playgrounds/Python/tft_v0/.venv/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 12. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:   0%|          | 0/13229 [00:00<?, ?it/s, v_num=0, train_loss_step=0.597, val_loss=0.938, train_loss_epoch=0.639]               "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_forecasting.data import TimeSeriesDataSet\n",
    "from pytorch_forecasting.models import TemporalFusionTransformer\n",
    "from pytorch_forecasting.metrics import MAE, MultiLoss\n",
    "from pytorch_forecasting.data.encoders import MultiNormalizer, TorchNormalizer\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n",
    "# --- 1. Device Setup ---\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Data Loading Function\n",
    "# -----------------------------\n",
    "def load_data(folder):\n",
    "    all_files = [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith('.csv')]\n",
    "    dfs = []\n",
    "    for file in all_files:\n",
    "        df = pd.read_csv(file, index_col=0)\n",
    "        df[\"time_idx\"] = range(len(df))\n",
    "        df[\"group\"] = os.path.basename(file).split('.')[0]\n",
    "        df[\"group\"] = df[\"group\"].astype(str)\n",
    "        df.rename(columns={\"Close\":\"target_1\",\"vclose\":\"target_2\"}, inplace=True)\n",
    "        dfs.append(df)\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Load Data\n",
    "# -----------------------------\n",
    "train_data_folder = \"data/train\"\n",
    "test_data_folder  = \"data/test\"\n",
    "oos_data_folder   = \"data/oos\"\n",
    "\n",
    "train_df = load_data(train_data_folder)\n",
    "test_df  = load_data(test_data_folder)\n",
    "oos_df   = load_data(oos_data_folder)\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Multi-Target Dataset\n",
    "# -----------------------------\n",
    "training = TimeSeriesDataSet(\n",
    "    train_df,\n",
    "    time_idx=\"time_idx\",\n",
    "    target=[\"target_1\", \"target_2\"],\n",
    "    group_ids=[\"group\"],\n",
    "    max_encoder_length=60,  # Adjust as needed\n",
    "    max_prediction_length=10,\n",
    "    static_categoricals=[\"group\"],\n",
    "    time_varying_known_reals=[\"time_idx\"],\n",
    "    time_varying_unknown_reals=[\n",
    "        c for c in train_df.columns\n",
    "        if c not in [\"group\",\"time_idx\"]  # \"target_1\",\"target_2\"\n",
    "    ],\n",
    "    target_normalizer=MultiNormalizer([\n",
    "        TorchNormalizer(method=\"identity\"),\n",
    "        TorchNormalizer(method=\"identity\")\n",
    "    ])\n",
    ")\n",
    "\n",
    "validation = TimeSeriesDataSet.from_dataset(training, test_df)\n",
    "oos        = TimeSeriesDataSet.from_dataset(training, oos_df)\n",
    "\n",
    "# -----------------------------\n",
    "# 5. DataLoaders\n",
    "# -----------------------------\n",
    "train_dataloader = training.to_dataloader(\n",
    "    train=True,  batch_size=16, shuffle=True,  num_workers=16, pin_memory=False\n",
    ")\n",
    "val_dataloader   = validation.to_dataloader(\n",
    "    train=False, batch_size=16, shuffle=False, num_workers=16, pin_memory=False\n",
    ")\n",
    "oos_dataloader   = oos.to_dataloader(\n",
    "    train=False, batch_size=16, shuffle=False, num_workers=16, pin_memory=False\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 6. TFT Model\n",
    "# -----------------------------\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=1e-4,\n",
    "    lstm_layers=2,\n",
    "    hidden_size=128,  # 256\n",
    "    attention_head_size=16, #64\n",
    "    dropout=0.2,\n",
    "    hidden_continuous_size=32, # 64\n",
    "    output_size=[1, 1],        # two targets\n",
    "    loss=MultiLoss([MAE(), MAE()]),\n",
    "    log_interval=10,\n",
    "    reduce_on_plateau_patience=4,\n",
    ").to(DEVICE)\n",
    "\n",
    "print(f\"Params in network: {tft.size()/1e3:.1f}k\")\n",
    "\n",
    "# -----------------------------\n",
    "# 7. LightningModule\n",
    "# -----------------------------\n",
    "class TFTLightningModule(LightningModule):\n",
    "    def __init__(self, tft_model):\n",
    "        super().__init__()\n",
    "        self.tft_model = tft_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.tft_model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        out = self(x)\n",
    "        pred = out[\"prediction\"]\n",
    "        loss = self.tft_model.loss(pred, y)\n",
    "        # log train loss at each step & epoch\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        out = self(x)\n",
    "        pred = out[\"prediction\"]\n",
    "        loss = self.tft_model.loss(pred, y)\n",
    "        # log val loss at each step & epoch\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return self.tft_model.configure_optimizers()\n",
    "\n",
    "tft_module = TFTLightningModule(tft).to(DEVICE)\n",
    "\n",
    "# -----------------------------\n",
    "# 8. Callbacks\n",
    "# -----------------------------\n",
    "# EarlyStopping\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=5,          # stop if no improvement for 5 epochs\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "# ModelCheckpoint\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints/\",                # folder to save checkpoints\n",
    "    filename=\"my-tft-{epoch:02d}-{val_loss:.2f}\",\n",
    "    save_top_k=1,                          # only keep the best model\n",
    "    monitor=\"val_loss\",                    # track val_loss\n",
    "    mode=\"min\",                            # lower val_loss is better\n",
    "    every_n_epochs=1                       # checkpoint every epoch\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 9. Trainer\n",
    "# -----------------------------\n",
    "trainer = Trainer(\n",
    "    max_epochs=30,\n",
    "    accelerator=\"gpu\" if DEVICE==\"cuda\" else \"cpu\",\n",
    "    devices=1,\n",
    "    precision=32,  # force float32\n",
    "    logger=CSVLogger(\"logs\", name=\"tft_multi_target_ckpt\"),\n",
    "    callbacks=[early_stop_callback, checkpoint_callback],\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 10. Fit (TRAIN)\n",
    "# -----------------------------\n",
    "trainer.fit(tft_module, train_dataloader, val_dataloader)\n",
    "\n",
    "# after training, best checkpoint is saved in \"checkpoints/\"\n",
    "\n",
    "# -----------------------------\n",
    "# 11. Save Model State Dict\n",
    "# -----------------------------\n",
    "# (Optional) we do a final explicit save\n",
    "torch.save(tft_module.state_dict(), \"tft_model_multi_target.pth\")\n",
    "\n",
    "# -----------------------------\n",
    "# 12. Predict\n",
    "# -----------------------------\n",
    "tft_module.eval()\n",
    "predictions = []\n",
    "for batch in oos_dataloader:\n",
    "    x, y = batch\n",
    "    out = tft_module(x)\n",
    "    preds = out[\"prediction\"].detach().cpu()\n",
    "    predictions.append(preds)\n",
    "\n",
    "predictions = torch.cat(predictions, dim=0)\n",
    "print(\"Predictions shape:\", predictions.shape)\n",
    "\n",
    "# done!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantile Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alecjeffery/anaconda3/envs/new_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/Users/alecjeffery/anaconda3/envs/new_env/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "/Users/alecjeffery/anaconda3/envs/new_env/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "/Users/alecjeffery/anaconda3/envs/new_env/lib/python3.11/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/__init__.py:171: UserWarning: In pytorch-forecasting models, on versions 1.1.X, the default optimizer defaults to 'adam', if pytorch_optimizer is not installed, otherwise it defaults to 'ranger' from pytorch_optimizer. From version 1.2.0, the default optimizer will be 'adam' regardless of whether pytorch_optimizer is installed, in order to minimize the number of dependencies in default parameter settings. Users who wish to ensure their code continues using 'ranger' as optimizer should ensure that pytorch_optimizer is installed, and set the optimizer parameter explicitly to 'ranger'.\n",
      "  super().__init__(loss=loss, logging_metrics=logging_metrics, **kwargs)\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/alecjeffery/anaconda3/envs/new_env/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/alecjeffery/anaconda3/envs/new_env/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /Users/alecjeffery/Documents/Playgrounds/Python/tft_v0/checkpoints exists and is not empty.\n",
      "\n",
      "  | Name      | Type                      | Params | Mode \n",
      "----------------------------------------------------------------\n",
      "0 | tft_model | TemporalFusionTransformer | 2.3 M  | train\n",
      "----------------------------------------------------------------\n",
      "2.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.3 M     Total params\n",
      "9.148     Total estimated model params size (MB)\n",
      "1184      Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of params in network: 2287.1k\n",
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alecjeffery/anaconda3/envs/new_env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:420: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n",
      "python(26762) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(26780) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(26781) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(26795) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(26813) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(26827) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(26829) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(26843) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(26862) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(26877) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(26892) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(26893) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(26908) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(26924) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(26943) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(26957) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alecjeffery/anaconda3/envs/new_env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:420: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n",
      "python(26971) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(26985) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(26988) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(27002) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(27016) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(27018) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(27034) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(27053) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(27067) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(27068) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(27082) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(27098) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(27100) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(27134) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(27174) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(27189) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  91%| | 3004/3307 [42:31<04:17,  1.18it/s, v_num=2, train_loss_step=0.453]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_forecasting.data import TimeSeriesDataSet\n",
    "from pytorch_forecasting.models import TemporalFusionTransformer\n",
    "from pytorch_forecasting.metrics import MultiLoss, QuantileLoss\n",
    "from pytorch_forecasting.data.encoders import MultiNormalizer, TorchNormalizer\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Device Setup\n",
    "# -----------------------------\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_float32_matmul_precision(\"medium\")  # Optimize for NVIDIA Tensor Cores\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Data Loading\n",
    "# -----------------------------\n",
    "def load_data(folder):\n",
    "    all_files = [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith('.csv')]\n",
    "    dfs = []\n",
    "    for file in all_files:\n",
    "        df = pd.read_csv(file, index_col=0)\n",
    "        df[\"time_idx\"] = range(len(df))\n",
    "        # group = ticker from filename\n",
    "        df[\"group\"] = os.path.basename(file).split('.')[0]\n",
    "        df[\"group\"] = df[\"group\"].astype(str)\n",
    "        # rename Close -> target_1, vclose -> target_2\n",
    "        df.rename(columns={\"Close\":\"target_1\",\"vclose\":\"target_2\"}, inplace=True)\n",
    "        dfs.append(df)\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Load datasets\n",
    "train_data_folder = \"data/train\"\n",
    "test_data_folder  = \"data/test\"\n",
    "oos_data_folder   = \"data/oos\"\n",
    "\n",
    "train_df = load_data(train_data_folder)\n",
    "test_df  = load_data(test_data_folder)\n",
    "oos_df   = load_data(oos_data_folder)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Multi-Target Dataset\n",
    "# -----------------------------\n",
    "training = TimeSeriesDataSet(\n",
    "    train_df,\n",
    "    time_idx=\"time_idx\",\n",
    "    target=[\"target_1\", \"target_2\"],\n",
    "    group_ids=[\"group\"],\n",
    "    max_encoder_length=65,\n",
    "    max_prediction_length=5,\n",
    "    static_categoricals=[\"group\"],\n",
    "    time_varying_known_reals=[\"time_idx\"],\n",
    "    time_varying_unknown_reals=[\n",
    "        c for c in train_df.columns\n",
    "        if c not in [\"group\",\"time_idx\",\"target_1\",\"target_2\"]\n",
    "    ],\n",
    "    # each target has its own normalizer\n",
    "    target_normalizer=MultiNormalizer([\n",
    "        TorchNormalizer(method=\"identity\"),  # for target_1\n",
    "        TorchNormalizer(method=\"identity\")   # for target_2\n",
    "    ])\n",
    ")\n",
    "\n",
    "validation = TimeSeriesDataSet.from_dataset(training, test_df)\n",
    "oos = TimeSeriesDataSet.from_dataset(training, oos_df)\n",
    "\n",
    "# -----------------------------\n",
    "# 4. DataLoaders\n",
    "# -----------------------------\n",
    "train_dataloader = training.to_dataloader(\n",
    "    train=True, batch_size=64, shuffle=True,\n",
    "    num_workers=16, pin_memory=False\n",
    ")\n",
    "val_dataloader = validation.to_dataloader(\n",
    "    train=False, batch_size=16, shuffle=False,\n",
    "    num_workers=16, pin_memory=False\n",
    ")\n",
    "oos_dataloader = oos.to_dataloader(\n",
    "    train=False, batch_size=16, shuffle=False,\n",
    "    num_workers=16, pin_memory=False\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Define Multi-Target TFT Model w/ Quantile Loss\n",
    "#    -> multiple quantiles for each target\n",
    "# -----------------------------\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.0002884031503126605,\n",
    "    lstm_layers=2,\n",
    "    hidden_size=64,\n",
    "    attention_head_size=4,\n",
    "    dropout=0.3,\n",
    "    hidden_continuous_size=64,\n",
    "    # For each target, we do 3 quantiles: 0.1, 0.5, 0.9\n",
    "    output_size=[3, 3],  # (3 quantiles for target_1, 3 quantiles for target_2)\n",
    "    loss=MultiLoss([\n",
    "        QuantileLoss(quantiles=[0.1, 0.5, 0.9]),\n",
    "        QuantileLoss(quantiles=[0.1, 0.5, 0.9])\n",
    "    ]),\n",
    "    log_interval=100,\n",
    "    reduce_on_plateau_patience=4,\n",
    ").to(DEVICE)\n",
    "\n",
    "print(f\"Number of params in network: {tft.size() / 1e3:.1f}k\")\n",
    "\n",
    "# -----------------------------\n",
    "# 6. LightningModule\n",
    "# -----------------------------\n",
    "class TFTLightningModule(LightningModule):\n",
    "    def __init__(self, tft_model):\n",
    "        super().__init__()\n",
    "        self.tft_model = tft_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.tft_model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        out = self(x)\n",
    "        pred = out[\"prediction\"]\n",
    "        loss = self.tft_model.loss(pred, y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        out = self(x)\n",
    "        pred = out[\"prediction\"]\n",
    "        loss = self.tft_model.loss(pred, y)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return self.tft_model.configure_optimizers()\n",
    "\n",
    "tft_module = TFTLightningModule(tft).to(DEVICE)\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Callbacks for EarlyStopping & ModelCheckpoint\n",
    "# -----------------------------\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=5,\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints/\",\n",
    "    filename=\"my-tft-{epoch:02d}-{val_loss:.2f}\",\n",
    "    save_top_k=1,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 8. Trainer\n",
    "# -----------------------------\n",
    "trainer = Trainer(\n",
    "    max_epochs=30,\n",
    "    accelerator=\"gpu\" if DEVICE == \"cuda\" else \"cpu\",\n",
    "    devices=1,\n",
    "    precision=32,  # float32 to avoid overflow\n",
    "    logger=CSVLogger(\"logs\", name=\"tft_multi_target_quantile\"),\n",
    "    callbacks=[early_stop_callback, checkpoint_callback],\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 9. Train\n",
    "# -----------------------------\n",
    "trainer.fit(tft_module, train_dataloader, val_dataloader)\n",
    "\n",
    "#  Best checkpoint is saved in \"checkpoints/\" folder if training is interrupted or completes\n",
    "\n",
    "# -----------------------------\n",
    "# 10. (Optional) Save State Dict\n",
    "# -----------------------------\n",
    "torch.save(tft_module.state_dict(), \"final_quantile_tft.pth\")\n",
    "\n",
    "# -----------------------------\n",
    "# 11. Predict on OOS\n",
    "# -----------------------------\n",
    "# Set the model to evaluation mode\n",
    "tft_module.eval()\n",
    "\n",
    "# Create an empty list to collect predictions for each batch\n",
    "all_preds = []\n",
    "\n",
    "for batch in oos_dataloader:\n",
    "    x, y = batch\n",
    "    out = tft_module(x)\n",
    "    \n",
    "    # Handle multi-target predictions:\n",
    "    # If out[\"prediction\"] is a list, process each element.\n",
    "    preds = out[\"prediction\"]\n",
    "    if isinstance(preds, list):\n",
    "        # Option 1: Stack predictions for each target into a single tensor.\n",
    "        # This gives you a tensor of shape (num_targets, batch_size, ...)\n",
    "        preds = torch.stack([p.detach().cpu() for p in preds], dim=0)\n",
    "        # Optionally, if you only care about one target, e.g. the first:\n",
    "        # preds = preds[0].detach().cpu()\n",
    "    else:\n",
    "        preds = preds.detach().cpu()\n",
    "    \n",
    "    all_preds.append(preds)\n",
    "\n",
    "# Depending on your stacking approach, you may then want to concatenate along the batch dimension.\n",
    "# For example, if each element is a tensor of shape (num_targets, batch_size, output_dim),\n",
    "# you can do:\n",
    "final_preds = torch.cat(all_preds, dim=1)  # Concatenate along the batch dimension.\n",
    "print(\"Final predictions shape:\", final_preds.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LR finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alecjeffery/anaconda3/envs/new_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/Users/alecjeffery/anaconda3/envs/new_env/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "/Users/alecjeffery/anaconda3/envs/new_env/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "/Users/alecjeffery/anaconda3/envs/new_env/lib/python3.11/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/__init__.py:171: UserWarning: In pytorch-forecasting models, on versions 1.1.X, the default optimizer defaults to 'adam', if pytorch_optimizer is not installed, otherwise it defaults to 'ranger' from pytorch_optimizer. From version 1.2.0, the default optimizer will be 'adam' regardless of whether pytorch_optimizer is installed, in order to minimize the number of dependencies in default parameter settings. Users who wish to ensure their code continues using 'ranger' as optimizer should ensure that pytorch_optimizer is installed, and set the optimizer parameter explicitly to 'ranger'.\n",
      "  super().__init__(loss=loss, logging_metrics=logging_metrics, **kwargs)\n",
      "Seed set to 42\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/alecjeffery/anaconda3/envs/new_env/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in network: 2287.1k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alecjeffery/anaconda3/envs/new_env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:420: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n",
      "python(24012) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(24032) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(24046) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(24061) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(24066) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(24080) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(24097) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(24121) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(24137) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(24180) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(24196) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(24215) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(24230) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(24232) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(24246) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(24260) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "/Users/alecjeffery/anaconda3/envs/new_env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:420: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n",
      "Finding best initial lr: 100%|| 200/200 [04:01<00:00,  1.21it/s]`Trainer.fit` stopped: `max_steps=200` reached.\n",
      "Finding best initial lr: 100%|| 200/200 [04:01<00:00,  1.21s/it]\n",
      "Learning rate set to 0.0002884031503126605\n",
      "Restoring states from the checkpoint path at /Users/alecjeffery/Documents/Playgrounds/Python/tft_v0/.lr_find_64ec43f6-43a5-4501-9609-25352bdc3ec5.ckpt\n",
      "Restored all states from the checkpoint at /Users/alecjeffery/Documents/Playgrounds/Python/tft_v0/.lr_find_64ec43f6-43a5-4501-9609-25352bdc3ec5.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR finder finished with an exception (expected): \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Trainer is already configured with a `LearningRateFinder` callback.Please remove it if you want to use the Tuner.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_TunerExitException\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 163\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     lr_find_result \u001b[38;5;241m=\u001b[39m \u001b[43mtuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr_find\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtft_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_training\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# specify training steps to run\u001b[39;49;00m\n\u001b[1;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/new_env/lib/python3.11/site-packages/lightning/pytorch/tuner/tuning.py:180\u001b[0m, in \u001b[0;36mTuner.lr_find\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, dataloaders, datamodule, method, min_lr, max_lr, num_training, mode, early_stop_threshold, update_attr, attr_name)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer\u001b[38;5;241m.\u001b[39mcallbacks \u001b[38;5;241m=\u001b[39m [lr_finder_callback] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer\u001b[38;5;241m.\u001b[39mcallbacks\n\u001b[0;32m--> 180\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer\u001b[38;5;241m.\u001b[39mcallbacks \u001b[38;5;241m=\u001b[39m [cb \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer\u001b[38;5;241m.\u001b[39mcallbacks \u001b[38;5;28;01mif\u001b[39;00m cb \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lr_finder_callback]\n",
      "File \u001b[0;32m~/anaconda3/envs/new_env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:539\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 539\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/new_env/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[0;32m~/anaconda3/envs/new_env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:575\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    569\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    571\u001b[0m     ckpt_path,\n\u001b[1;32m    572\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    574\u001b[0m )\n\u001b[0;32m--> 575\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n",
      "File \u001b[0;32m~/anaconda3/envs/new_env/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:962\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;241m==\u001b[39m TrainerFn\u001b[38;5;241m.\u001b[39mFITTING:\n\u001b[0;32m--> 962\u001b[0m     \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_callback_hooks\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mon_fit_start\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    963\u001b[0m     call\u001b[38;5;241m.\u001b[39m_call_lightning_module_hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_fit_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/new_env/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:222\u001b[0m, in \u001b[0;36m_call_callback_hooks\u001b[0;34m(trainer, hook_name, monitoring_callbacks, *args, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Callback]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcallback\u001b[38;5;241m.\u001b[39mstate_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 222\u001b[0m             \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pl_module:\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/new_env/lib/python3.11/site-packages/lightning/pytorch/callbacks/lr_finder.py:130\u001b[0m, in \u001b[0;36mLearningRateFinder.on_fit_start\u001b[0;34m(self, trainer, pl_module)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mon_fit_start\u001b[39m(\u001b[38;5;28mself\u001b[39m, trainer: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.Trainer\u001b[39m\u001b[38;5;124m\"\u001b[39m, pl_module: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr_find\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpl_module\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/new_env/lib/python3.11/site-packages/lightning/pytorch/callbacks/lr_finder.py:126\u001b[0m, in \u001b[0;36mLearningRateFinder.lr_find\u001b[0;34m(self, trainer, pl_module)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_early_exit:\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _TunerExitException()\n",
      "\u001b[0;31m_TunerExitException\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 174\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLR finder finished with an exception (expected):\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Try to get a result without update_attr if necessary:\u001b[39;00m\n\u001b[0;32m--> 174\u001b[0m     lr_find_result \u001b[38;5;241m=\u001b[39m \u001b[43mtuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr_find\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtft_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_training\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mupdate_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m suggested_lr \u001b[38;5;241m=\u001b[39m lr_find_result\u001b[38;5;241m.\u001b[39msuggestion()\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuggested learning rate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuggested_lr\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/new_env/lib/python3.11/site-packages/lightning/pytorch/tuner/tuning.py:162\u001b[0m, in \u001b[0;36mTuner.lr_find\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, dataloaders, datamodule, method, min_lr, max_lr, num_training, mode, early_stop_threshold, update_attr, attr_name)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmethod=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the only valid configuration to run lr finder.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    161\u001b[0m _check_tuner_configuration(train_dataloaders, val_dataloaders, dataloaders, method)\n\u001b[0;32m--> 162\u001b[0m \u001b[43m_check_lr_find_configuration\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_trainer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# local import to avoid circular import\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlr_finder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LearningRateFinder\n",
      "File \u001b[0;32m~/anaconda3/envs/new_env/lib/python3.11/site-packages/lightning/pytorch/tuner/tuning.py:217\u001b[0m, in \u001b[0;36m_check_lr_find_configuration\u001b[0;34m(trainer)\u001b[0m\n\u001b[1;32m    215\u001b[0m configured_callbacks \u001b[38;5;241m=\u001b[39m [cb \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mcallbacks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(cb, LearningRateFinder)]\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m configured_callbacks:\n\u001b[0;32m--> 217\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrainer is already configured with a `LearningRateFinder` callback.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease remove it if you want to use the Tuner.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    220\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Trainer is already configured with a `LearningRateFinder` callback.Please remove it if you want to use the Tuner."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_forecasting.data import TimeSeriesDataSet\n",
    "from pytorch_forecasting.models import TemporalFusionTransformer, Baseline\n",
    "from pytorch_forecasting.metrics import MAE, MultiLoss, QuantileLoss\n",
    "from pytorch_forecasting.data.encoders import MultiNormalizer, TorchNormalizer\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Accelerator/Device Setup\n",
    "# -----------------------------\n",
    "if torch.cuda.is_available():\n",
    "    accelerator = \"gpu\"\n",
    "else:\n",
    "    accelerator = \"cpu\"\n",
    "DEVICE = accelerator  # Use this for Trainer and .to()\n",
    "\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Data Loading Function\n",
    "# -----------------------------\n",
    "def load_data(folder):\n",
    "    all_files = [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith('.csv')]\n",
    "    dfs = []\n",
    "    for file in all_files:\n",
    "        df = pd.read_csv(file, index_col=0)\n",
    "        df[\"time_idx\"] = range(len(df))\n",
    "        df[\"group\"] = os.path.basename(file).split('.')[0]\n",
    "        df[\"group\"] = df[\"group\"].astype(str)\n",
    "        df.rename(columns={\"Close\": \"target_1\", \"vclose\": \"target_2\"}, inplace=True)\n",
    "        dfs.append(df)\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Load Datasets\n",
    "# -----------------------------\n",
    "train_data_folder = \"data/train\"\n",
    "test_data_folder  = \"data/test\"\n",
    "oos_data_folder   = \"data/oos\"\n",
    "\n",
    "train_df = load_data(train_data_folder)\n",
    "test_df  = load_data(test_data_folder)\n",
    "oos_df   = load_data(oos_data_folder)\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Multi-Target Dataset\n",
    "# -----------------------------\n",
    "training = TimeSeriesDataSet(\n",
    "    train_df,\n",
    "    time_idx=\"time_idx\",\n",
    "    target=[\"target_1\", \"target_2\"],\n",
    "    group_ids=[\"group\"],\n",
    "    max_encoder_length=65,\n",
    "    max_prediction_length=5,\n",
    "    static_categoricals=[\"group\"],\n",
    "    time_varying_known_reals=[\"time_idx\"],\n",
    "    time_varying_unknown_reals=[\n",
    "        c for c in train_df.columns if c not in [\"group\", \"time_idx\", \"target_1\", \"target_2\"]\n",
    "    ],\n",
    "    target_normalizer=MultiNormalizer([\n",
    "        TorchNormalizer(method=\"identity\"),\n",
    "        TorchNormalizer(method=\"identity\")\n",
    "    ])\n",
    ")\n",
    "\n",
    "validation = TimeSeriesDataSet.from_dataset(training, test_df)\n",
    "oos = TimeSeriesDataSet.from_dataset(training, oos_df)\n",
    "\n",
    "# -----------------------------\n",
    "# 5. DataLoaders\n",
    "# -----------------------------\n",
    "train_dataloader = training.to_dataloader(\n",
    "    train=True, batch_size=64, shuffle=True, num_workers=16, pin_memory=False\n",
    ")\n",
    "val_dataloader = validation.to_dataloader(\n",
    "    train=False, batch_size=64, shuffle=False, num_workers=16, pin_memory=False\n",
    ")\n",
    "oos_dataloader = oos.to_dataloader(\n",
    "    train=False, batch_size=64, shuffle=False, num_workers=16, pin_memory=False\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Define Multi-Target TFT Model with Quantile Loss\n",
    "#    (Predicting 3 quantiles per target  output_size=[3, 3])\n",
    "# -----------------------------\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=1e-3,  # initial LR; will be tuned\n",
    "    lstm_layers=2,\n",
    "    hidden_size=64,\n",
    "    attention_head_size=4,\n",
    "    dropout=0.35,\n",
    "    hidden_continuous_size=64,\n",
    "    output_size=[3, 3],\n",
    "    loss=MultiLoss([\n",
    "        QuantileLoss(quantiles=[0.1, 0.5, 0.9]),\n",
    "        QuantileLoss(quantiles=[0.1, 0.5, 0.9])\n",
    "    ]),\n",
    "    log_interval=100,\n",
    "    reduce_on_plateau_patience=4,\n",
    ").to(DEVICE)\n",
    "\n",
    "print(f\"Number of parameters in network: {tft.size() / 1e3:.1f}k\")\n",
    "\n",
    "# -----------------------------\n",
    "# 7. LightningModule Wrapper\n",
    "# -----------------------------\n",
    "class TFTLightningModule(LightningModule):\n",
    "    def __init__(self, tft_model):\n",
    "        super().__init__()\n",
    "        self.tft_model = tft_model\n",
    "        # Save the learning rate in hparams for LR finder purposes\n",
    "        self.save_hyperparameters({'learning_rate': tft_model.hparams.learning_rate})\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.tft_model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        out = self(x)\n",
    "        pred = out[\"prediction\"]\n",
    "        loss = self.tft_model.loss(pred, y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        out = self(x)\n",
    "        pred = out[\"prediction\"]\n",
    "        loss = self.tft_model.loss(pred, y)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return self.tft_model.configure_optimizers()\n",
    "\n",
    "tft_module = TFTLightningModule(tft).to(DEVICE)\n",
    "\n",
    "# -----------------------------\n",
    "# 8. Learning Rate Finder\n",
    "# -----------------------------\n",
    "pl.seed_everything(42)\n",
    "# IMPORTANT: Create an LR finder trainer with an empty callbacks list.\n",
    "lr_finder_trainer = Trainer(\n",
    "    max_epochs=1,\n",
    "    accelerator=accelerator,\n",
    "    devices=1,\n",
    "    precision=32,\n",
    "    callbacks=[],  # ensure no LR finder callback is pre-configured\n",
    ")\n",
    "\n",
    "tuner = Tuner(lr_finder_trainer)\n",
    "try:\n",
    "    lr_find_result = tuner.lr_find(\n",
    "        tft_module,\n",
    "        train_dataloaders=train_dataloader,\n",
    "        val_dataloaders=val_dataloader,\n",
    "        max_lr=1.0,\n",
    "        min_lr=1e-6,\n",
    "        num_training=200,  # specify training steps to run\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"LR finder finished with an exception (expected):\", e)\n",
    "    # Try to get a result without update_attr if necessary:\n",
    "    lr_find_result = tuner.lr_find(\n",
    "        tft_module,\n",
    "        train_dataloaders=train_dataloader,\n",
    "        val_dataloaders=val_dataloader,\n",
    "        max_lr=1.0,\n",
    "        min_lr=1e-6,\n",
    "        num_training=200,\n",
    "        update_attr=False,\n",
    "    )\n",
    "\n",
    "suggested_lr = lr_find_result.suggestion()\n",
    "print(f\"Suggested learning rate: {suggested_lr:.2e}\")\n",
    "\n",
    "fig = lr_find_result.plot(suggest=True)\n",
    "fig.show()\n",
    "\n",
    "# (Optionally) Update the model's learning rate manually:\n",
    "# tft.hparams.learning_rate = suggested_lr\n",
    "\n",
    "# -----------------------------\n",
    "# 9. Final Trainer Setup & Training\n",
    "# -----------------------------\n",
    "# (Uncomment and run these blocks after you are satisfied with the LR finder)\n",
    "# early_stop_callback = EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\")\n",
    "# checkpoint_callback = ModelCheckpoint(\n",
    "#     dirpath=\"checkpoints/\",\n",
    "#     filename=\"my-tft-{epoch:02d}-{val_loss:.2f}\",\n",
    "#     save_top_k=1,\n",
    "#     monitor=\"val_loss\",\n",
    "#     mode=\"min\"\n",
    "# )\n",
    "#\n",
    "# final_trainer = Trainer(\n",
    "#     max_epochs=30,\n",
    "#     accelerator=accelerator,\n",
    "#     devices=1,\n",
    "#     precision=32,\n",
    "#     logger=CSVLogger(\"logs\", name=\"tft_multi_target_quantile_lr\"),\n",
    "#     callbacks=[early_stop_callback, checkpoint_callback],\n",
    "#     gradient_clip_val=0.1,\n",
    "# )\n",
    "#\n",
    "# final_trainer.fit(tft_module, train_dataloader, val_dataloader)\n",
    "# print(\"Training complete. Best checkpoint:\", checkpoint_callback.best_model_path)\n",
    "#\n",
    "# torch.save(tft_module.state_dict(), \"final_quantile_tft.pth\")\n",
    "#\n",
    "# tft_module.eval()\n",
    "# predictions = []\n",
    "# for batch in oos_dataloader:\n",
    "#     x, y = batch\n",
    "#     out = tft_module(x)\n",
    "#     pred = out[\"prediction\"].detach().cpu()\n",
    "#     predictions.append(pred)\n",
    "# predictions = torch.cat(predictions, dim=0)\n",
    "# print(\"Predictions shape:\", predictions.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Suppose lr_find_result.suggestion() returned 9.55e-05.\n",
    "# suggested_lr = 9.55e-05  # or however you determined it\n",
    "# print(f\"Using suggested learning rate: {suggested_lr:.2e}\")\n",
    "\n",
    "# # Update your model's learning rate manually\n",
    "# tft.hparams.learning_rate = suggested_lr\n",
    "\n",
    "# # Then run your final training routine with your final trainer:\n",
    "# final_trainer = Trainer(\n",
    "#     max_epochs=30,\n",
    "#     accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "#     devices=1,\n",
    "#     precision=32,\n",
    "#     logger=CSVLogger(\"logs\", name=\"tft_multi_target_quantile_lr\"),\n",
    "#     callbacks=[early_stop_callback, checkpoint_callback],\n",
    "#     gradient_clip_val=0.1,\n",
    "# )\n",
    "\n",
    "# final_trainer.fit(tft_module, train_dataloader, val_dataloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a model and run preditions\n",
    "\n",
    "\"final_quantile_tft.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to an error with the 'pyzmq' module. Consider re-installing this module.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresPyzmq'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "# if using a non-conda environment\n",
    "!python -m pip install ipykernel pyzmq -U --force-reinstall\n",
    "# if using a conda environment\n",
    "!conda install -c anaconda ipykernel pyzmq --name <environment name> --update-deps --force-reinstall"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
